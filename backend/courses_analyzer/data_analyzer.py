# courses_analyzer/data_analyzer.py
#!/usr/bin/env python3
"""
Script phÃ¢n tÃ­ch dá»¯ liá»‡u Udemy courses vÃ  lÆ°u vÃ o ChromaDB
KHÃ”NG sá»­ dá»¥ng OpenAI embedding - dÃ¹ng ChromaDB default
"""

import pandas as pd
import chromadb
from chromadb.config import Settings
import os
from dotenv import load_dotenv
import json
import time
from typing import List, Dict, Any, Tuple
import logging
import re
from pathlib import Path

# Cáº¥u hÃ¬nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

class UdemyCourseAnalyzer:
    def __init__(self):
        """Khá»Ÿi táº¡o analyzer - KHÃ”NG dÃ¹ng OpenAI embedding"""

        # Khá»Ÿi táº¡o ChromaDB
        self.chroma_path = os.getenv('CHROMA_DB_PATH', './chroma_db')
        self.collection_name = os.getenv('COLLECTION_NAME', 'udemy_courses')

        # Táº¡o ChromaDB client
        self.chroma_client = chromadb.PersistentClient(
            path=self.chroma_path
        )

        # XÃ³a collection cÅ© náº¿u tá»“n táº¡i vÃ  táº¡o má»›i
        try:
            self.chroma_client.delete_collection(self.collection_name)
            logger.info(f"ğŸ—‘ï¸ ÄÃ£ xÃ³a collection cÅ©: {self.collection_name}")
        except:
            logger.info(f"â„¹ï¸ KhÃ´ng cÃ³ collection cÅ© Ä‘á»ƒ xÃ³a: {self.collection_name}")

        # Táº¡o collection má»›i - ChromaDB sáº½ dÃ¹ng default embedding
        self.collection = self.chroma_client.create_collection(
            name=self.collection_name,
            metadata={"description": "Udemy courses data for AI chatbot"}
        )
        logger.info(f"âœ… ÄÃ£ táº¡o collection má»›i: {self.collection_name}")

    def clean_text(self, text: str) -> str:
        """LÃ m sáº¡ch text, xá»­ lÃ½ encoding issues"""
        if pd.isna(text) or text is None:
            return ""

        text = str(text)
        text = text.encode('utf-8', errors='ignore').decode('utf-8')
        text = re.sub(r'[^\w\s\.,!?\-\(\)\[\]:]', ' ', text)
        text = ' '.join(text.split())
        return text.strip()

    def parse_list_field(self, field_value: str) -> List[str]:
        """Parse cÃ¡c trÆ°á»ng dáº¡ng list tá»« CSV"""
        if pd.isna(field_value) or field_value is None:
            return []

        field_value = str(field_value).strip()

        if field_value.startswith('[') and field_value.endswith(']'):
            try:
                field_value = field_value.replace("'", '"')
                parsed_list = json.loads(field_value)
                return [self.clean_text(item) for item in parsed_list if item]
            except json.JSONDecodeError:
                pass

        items = re.split(r'[,;]', field_value.strip('[]'))
        return [self.clean_text(item) for item in items if item.strip()]

    def create_course_document(self, course_row: pd.Series) -> Tuple[str, Dict[str, Any]]:
        """Táº¡o document Ä‘Æ¡n giáº£n cho má»—i course"""
        # Táº¡o text document táº­p trung vÃ o keywords Ä‘á»ƒ search tá»‘t
        doc_parts = [
            f"COURSE TITLE: {course_row['Title']}",
            f"DESCRIPTION: {course_row['Detailed Description']}",
            f"INSTRUCTOR: {course_row['Instructor']}",
            f"LEVEL: {course_row['Level']}",
            f"RATING: {course_row['Rating']}",
            f"DURATION: {course_row['Duration']}",
        ]

        # ThÃªm learning outcomes - quan trá»ng cho search
        if course_row['What You\'ll Learn']:
            doc_parts.append("LEARNING OUTCOMES:")
            doc_parts.extend([f"- {item}" for item in course_row['What You\'ll Learn'][:10]])  # Giá»›i háº¡n 10 items

        # ThÃªm requirements
        if course_row['Requirements']:
            doc_parts.append("REQUIREMENTS:")
            doc_parts.extend([f"- {item}" for item in course_row['Requirements'][:5]])

        # ThÃªm target audience
        if course_row['Target Audience']:
            doc_parts.append("TARGET AUDIENCE:")
            doc_parts.extend([f"- {item}" for item in course_row['Target Audience'][:5]])

        document_text = "\n".join(doc_parts)

        # Táº¡o metadata phong phÃº Ä‘á»ƒ filter
        metadata = {
            'title': course_row['Title'],
            'instructor': course_row['Instructor'],
            'level': course_row['Level'].lower() if course_row['Level'] else 'all levels',
            'rating': float(course_row['Rating']) if pd.notna(course_row['Rating']) else 0.0,
            'duration': course_row['Duration'],
            'link': course_row['Link'],
            'price': getattr(course_row, 'Current Price', 'Free'),
            'skills': ', '.join(self.extract_keywords_from_title(course_row['Title']))
        }

        return document_text, metadata

    def extract_keywords_from_title(self, title: str) -> List[str]:
        """Extract key technology/topic keywords from course title"""
        if not title:
            return []

        tech_keywords = {
            'python', 'javascript', 'java', 'react', 'angular', 'vue', 'node', 'nodejs',
            'django', 'flask', 'laravel', 'spring', 'express', 'mongodb', 'mysql',
            'postgresql', 'html', 'css', 'typescript', 'php', 'ruby', 'go', 'rust',
            'docker', 'kubernetes', 'aws', 'azure', 'gcp', 'machine learning', 'ml',
            'artificial intelligence', 'ai', 'data science', 'blockchain', 'flutter',
            'swift', 'kotlin', 'android', 'ios', 'unity', 'tensorflow', 'pytorch',
            'backend', 'frontend', 'fullstack', 'web development', 'mobile development',
            'cloud', 'devops', 'database', 'sql', 'nosql'
        }

        title_lower = title.lower()
        found_keywords = [keyword for keyword in tech_keywords if keyword in title_lower]

        return found_keywords[:8]  # Limit to 8 keywords

    def load_and_process_data(self, csv_file_path: str) -> pd.DataFrame:
        """Load vÃ  xá»­ lÃ½ dá»¯ liá»‡u tá»« file CSV"""
        logger.info(f"Äang load dá»¯ liá»‡u tá»«: {csv_file_path}")

        try:
            df = pd.read_csv(csv_file_path, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(csv_file_path, encoding='latin-1')
            except UnicodeDecodeError:
                df = pd.read_csv(csv_file_path, encoding='cp1252')

        logger.info(f"âœ… ÄÃ£ load {len(df)} courses tá»« CSV")

        # LÃ m sáº¡ch dá»¯ liá»‡u cÆ¡ báº£n
        df['Title'] = df['Title'].apply(self.clean_text)
        df['Detailed Description'] = df['Detailed Description'].apply(self.clean_text)
        df['Instructor'] = df['Instructor'].apply(self.clean_text)
        df['Level'] = df['Level'].apply(self.clean_text)
        df['Duration'] = df['Duration'].apply(self.clean_text)

        # Parse cÃ¡c trÆ°á»ng list
        df['What You\'ll Learn'] = df['What You\'ll Learn'].apply(self.parse_list_field)
        df['Requirements'] = df['Requirements'].apply(self.parse_list_field)
        df['Target Audience'] = df['Target Audience'].apply(self.parse_list_field)

        # Xá»­ lÃ½ rating
        df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce').fillna(0.0)

        return df

    def process_and_store_courses_fast(self, df: pd.DataFrame, sample_size: int = None):
        """Xá»­ lÃ½ vÃ  lÆ°u courses vÃ o ChromaDB - KHÃ”NG dÃ¹ng embedding, ráº¥t nhanh"""
        if sample_size:
            df = df.head(sample_size)
            logger.info(f"ğŸ§ª Äang xá»­ lÃ½ {sample_size} courses máº«u...")
        else:
            logger.info(f"ğŸš€ Äang xá»­ lÃ½ {len(df)} courses...")

        documents = []
        metadatas = []
        ids = []

        successful = 0

        for course_idx, row in df.iterrows():
            try:
                if course_idx % 50 == 0:  # Log Ã­t hÆ¡n Ä‘á»ƒ Ä‘á»¡ spam
                    logger.info(f"ğŸ“ Äang xá»­ lÃ½ course {course_idx + 1}/{len(df)}...")

                # Táº¡o document
                document_text, metadata = self.create_course_document(row)

                # Táº¡o ID
                course_id = f"course_{course_idx}"

                documents.append(document_text)
                metadatas.append(metadata)
                ids.append(course_id)

                successful += 1

                # LÆ°u theo batch Ä‘á»ƒ trÃ¡nh memory issues
                if len(documents) >= 100:
                    self.collection.add(
                        documents=documents,
                        metadatas=metadatas,
                        ids=ids
                    )
                    logger.info(f"âœ… ÄÃ£ lÆ°u batch {len(documents)} courses")
                    documents, metadatas, ids = [], [], []

            except Exception as e:
                logger.error(f"âŒ Lá»—i khi xá»­ lÃ½ course {course_idx}: {e}")
                continue

        # LÆ°u batch cuá»‘i cÃ¹ng
        if documents:
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            logger.info(f"âœ… ÄÃ£ lÆ°u batch cuá»‘i {len(documents)} courses")

        logger.info(f"ğŸ‰ HoÃ n thÃ nh! ÄÃ£ lÆ°u {successful} courses vÃ o ChromaDB")
        return successful

    def get_collection_info(self):
        """Hiá»ƒn thá»‹ thÃ´ng tin collection"""
        count = self.collection.count()
        logger.info(f"ğŸ“Š Collection '{self.collection_name}' cÃ³ {count} documents")
        return count

def main():
    """HÃ m main Ä‘á»ƒ cháº¡y script"""
    print("ğŸš€ Udemy Course Data Analyzer - Fast Version (No Embedding)")
    print("=" * 60)

    # Khá»Ÿi táº¡o analyzer
    analyzer = UdemyCourseAnalyzer()

    # File CSV path
    csv_file = Path(__file__).resolve().parent.parent / "data" / "UDEMY_2025.csv"

    if not os.path.exists(csv_file):
        logger.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {csv_file}")
        return

    # Load dá»¯ liá»‡u
    df = analyzer.load_and_process_data(csv_file)

    print(f"\nğŸ“Š Tá»•ng sá»‘ courses: {len(df)}")
    print(f"ğŸ¯ 3 courses Ä‘áº§u tiÃªn:")
    for idx, row in df.head(3).iterrows():
        print(f"  {idx + 1}. {row['Title'][:80]}...")

    # Há»i sample size
    print(f"\nâš¡ PHÆ¯Æ NG PHÃP NHANH: KhÃ´ng dÃ¹ng OpenAI Embedding")
    print(f"   ChromaDB sáº½ tá»± Ä‘á»™ng xá»­ lÃ½ text search")
    print(f"\nğŸ§ª Báº¡n muá»‘n xá»­ lÃ½ bao nhiÃªu courses?")
    print("   (Nháº­p sá»‘, hoáº·c 'all' Ä‘á»ƒ xá»­ lÃ½ táº¥t cáº£)")
    response = input("   Sá»‘ lÆ°á»£ng: ").strip().lower()

    sample_size = None
    if response != 'all' and response.isdigit():
        sample_size = min(int(response), len(df))
        print(f"   âœ… Sáº½ xá»­ lÃ½ {sample_size} courses")
    else:
        sample_size = len(df)
        print(f"   âœ… Sáº½ xá»­ lÃ½ táº¥t cáº£ {sample_size} courses")

    # XÃ¡c nháº­n
    confirm = input(f"\nğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ NHANH? (y/n): ").strip().lower()
    if confirm != 'y':
        print("âŒ ÄÃ£ há»§y")
        return

    # Xá»­ lÃ½
    print(f"\nâ³ Äang xá»­ lÃ½ NHANH (khÃ´ng dÃ¹ng embedding)...")
    start_time = time.time()
    successful = analyzer.process_and_store_courses_fast(df, sample_size)
    end_time = time.time()

    # Káº¿t quáº£
    final_count = analyzer.get_collection_info()

    print(f"\nğŸ‰ HOÃ€N THÃ€NH SIÃŠU NHANH!")
    print(f"â±ï¸  Thá»i gian: {end_time - start_time:.2f} giÃ¢y")
    print(f"ğŸ“Š Courses processed: {successful}")
    print(f"ğŸ’¾ ChromaDB: {analyzer.chroma_path}")
    print(f"ğŸ“š Collection: {analyzer.collection_name}")
    print(f"âœ… Documents: {final_count}")

    # Test query
    print(f"\nğŸ” Testing search...")
    try:
        test_results = analyzer.collection.query(
            query_texts=["python backend development"],
            n_results=3
        )
        if test_results['documents']:
            print(f"âœ… Search test thÃ nh cÃ´ng! TÃ¬m tháº¥y {len(test_results['documents'][0])} káº¿t quáº£")
        else:
            print("âš ï¸ Search test khÃ´ng cÃ³ káº¿t quáº£")
    except Exception as e:
        print(f"âŒ Search test lá»—i: {e}")

if __name__ == "__main__":
    main()
